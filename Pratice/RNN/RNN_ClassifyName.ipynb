{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Italian.txt', 'data/names/Irish.txt', 'data/names/Russian.txt', 'data/names/Portuguese.txt', 'data/names/Arabic.txt', 'data/names/Czech.txt', 'data/names/Scottish.txt', 'data/names/Vietnamese.txt', 'data/names/Chinese.txt', 'data/names/Korean.txt', 'data/names/Polish.txt', 'data/names/Japanese.txt', 'data/names/Spanish.txt', 'data/names/English.txt', 'data/names/German.txt', 'data/names/Dutch.txt', 'data/names/Greek.txt', 'data/names/French.txt']\n"
     ]
    }
   ],
   "source": [
    "def findFiles(path):\n",
    "    return glob.glob(path) #return all files paths that match a specific pattern\n",
    "\n",
    "print(findFiles('data/names/*.txt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truong Minh Hong\n"
     ]
    }
   ],
   "source": [
    "import unicodedata, string\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "print(unicodeToAscii('Trương Minh Hồng'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_lines = dict()\n",
    "all_categories = list()\n",
    "\n",
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [unicodeToAscii(line) for line in lines]\n",
    "\n",
    "for filename in findFiles('data/names/*.txt'):\n",
    "    category = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(category)\n",
    "    lines = readLines(filename)\n",
    "    category_lines[category] = lines\n",
    "\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Italian', 'Irish', 'Russian', 'Portuguese', 'Arabic', 'Czech', 'Scottish', 'Vietnamese', 'Chinese', 'Korean', 'Polish', 'Japanese', 'Spanish', 'English', 'German', 'Dutch', 'Greek', 'French']\n",
      "['Nguyen', 'Tron', 'Le', 'Pham', 'Huynh']\n"
     ]
    }
   ],
   "source": [
    "print(all_categories)\n",
    "print(category_lines['Vietnamese'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m977.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:08\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions\n",
      "  Downloading typing_extensions-4.3.0-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: typing-extensions, torch\n",
      "Successfully installed torch-1.12.1 typing-extensions-4.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(size = (1, n_letters))\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(size = (len(line), 1, n_letters))  #1 for batch\n",
    "    for index, letter in enumerate(line):\n",
    "        tensor[index][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, inputSize, hiddenSize, outputSize):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.i2h = nn.Linear(inputSize+hiddenSize, hiddenSize)\n",
    "        self.i2o = nn.Linear(inputSize+hiddenSize, outputSize)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(size = (1, self.hiddenSize))\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden =  self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "model = RNN(n_letters, n_hidden, n_categories)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (i2h): Linear(in_features=185, out_features=128, bias=True)\n",
       "  (i2o): Linear(in_features=185, out_features=18, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(line, target):\n",
    "    hidden = model.initHidden()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    for i in range(line.size()[0]):\n",
    "        output, hidden = model(line[i], hidden)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return output, loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category lines --> dictionary (languages: list of words)\n",
    "#all_categories --> list of languages\n",
    "import random\n",
    "\n",
    "def randomTraining():\n",
    "    #Get random language\n",
    "    langIndex = random.randint(0, len(all_categories) - 1)\n",
    "    category = all_categories[langIndex]\n",
    "\n",
    "    #convert target language to one-hot\n",
    "    targetTensor = torch.zeros(size = (1, len(all_categories)))\n",
    "    targetTensor[0][langIndex] = 1\n",
    "\n",
    "    #Get random word from that language\n",
    "    wordIndex = random.randint(0, len(category_lines[category]) - 1)\n",
    "    word = category_lines[category][wordIndex]\n",
    "    inputTensor = lineToTensor(word)\n",
    "    return word, inputTensor, category, targetTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category: Chinese, word: Jia\n",
      "inputTensor: torch.Size([3, 1, 57])\n",
      "targetTensor: torch.Size([1, 18])\n",
      "********************\n",
      "category: Dutch, word: Aalst\n",
      "inputTensor: torch.Size([5, 1, 57])\n",
      "targetTensor: torch.Size([1, 18])\n",
      "********************\n",
      "category: French, word: Soucy\n",
      "inputTensor: torch.Size([5, 1, 57])\n",
      "targetTensor: torch.Size([1, 18])\n",
      "********************\n",
      "category: Japanese, word: Shiba\n",
      "inputTensor: torch.Size([5, 1, 57])\n",
      "targetTensor: torch.Size([1, 18])\n",
      "********************\n",
      "category: Portuguese, word: Palmeiro\n",
      "inputTensor: torch.Size([8, 1, 57])\n",
      "targetTensor: torch.Size([1, 18])\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    word, inputTensor, category, targetTensor = randomTraining()\n",
    "    print(f'category: {category}, word: {word}')\n",
    "    print(f'inputTensor: {inputTensor.shape}')\n",
    "    print(f'targetTensor: {targetTensor.shape}')\n",
    "    print('*'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 100000\n",
    "Loss = list()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    word, inputTensor, category, targetTensor = randomTraining()\n",
    "    outputTensor, loss = train(inputTensor, targetTensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
